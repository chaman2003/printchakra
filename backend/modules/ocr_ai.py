"""
OCR & AI Recognition Module - Improved multi-config OCR extraction
Text extraction with multiple preprocessing variants and configurations
Based on printchakra_clean.ipynb Section 5
"""

import cv2
import numpy as np
import pytesseract
from typing import Dict, List, Tuple, Optional
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import pickle
import os


class OCRModule:
    """
    Handles OCR text extraction with multi-configuration approach
    Based on printchakra_clean.ipynb Section 5
    """
    
    def __init__(self, language: str = 'eng', psm: int = 3, oem: int = 3):
        """
        Initialize OCR module
        
        Args:
            language: Tesseract language code (e.g., 'eng', 'eng+fra')
            psm: Page segmentation mode (0-13)
            oem: OCR Engine mode (0-3)
                0 = Legacy engine only
                1 = Neural nets LSTM engine only
                2 = Legacy + LSTM engines
                3 = Default (based on what is available)
        """
        self.language = language
        self.psm = psm
        self.oem = oem
        self.config = f'--oem {oem} --psm {psm}'
    
    def extract_text_multi_config(self, image: np.ndarray, debug: bool = False) -> Tuple[str, Dict]:
        """
        Extract text with multiple OCR configurations and preprocessing variants
        From notebook Section 5 - improved multi-config approach
        
        Args:
            image: Input image
            debug: Print debug information
            
        Returns:
            Tuple of (best_text, statistics_dict)
        """
        from .image_enhancement import ImageEnhancer
        
        configs = [
            ('PSM 3 (Auto)', '--oem 3 --psm 3'),
            ('PSM 6 (Block)', '--oem 3 --psm 6'),
            ('PSM 4 (Column)', '--oem 3 --psm 4')
        ]
        
        # Get preprocessing variants
        enhancer = ImageEnhancer()
        variants = enhancer.preprocess_for_ocr(image)
        results = []
        
        try:
            for img, variant_name in variants:
                for config_name, config_str in configs:
                    text = pytesseract.image_to_string(img, lang=self.language, config=config_str)
                    results.append({
                        'text': text,
                        'length': len(text.strip()),
                        'config': config_name,
                        'variant': variant_name
                    })
            
            # Select best result (longest text)
            best = max(results, key=lambda x: x['length'])
            
            if debug:
                print(f"âœ… Best OCR: {best['config']} ({best['variant']}) - {best['length']} chars")
            
            # Get detailed stats from first variant
            data = pytesseract.image_to_data(variants[0][0], output_type=pytesseract.Output.DICT)
            words = [w for w in data['text'] if w.strip()]
            lines = set(data['line_num'])
            
            stats = {
                'chars': len(best['text']),
                'words': len(words),
                'lines': len(lines),
                'config': best['config'],
                'variant': best['variant']
            }
            
            return best['text'], stats
            
        except Exception as e:
            if debug:
                print(f"OCR error: {e}")
            return "", {}
    
    def extract_text(self, image: np.ndarray) -> str:
        """
        Extract text from image (simple version for compatibility)
        
        Args:
            image: Input image
            
        Returns:
            Extracted text
        """
        try:
            text = pytesseract.image_to_string(
                image, 
                lang=self.language, 
                config=self.config
            )
            return text.strip()
        except Exception as e:
            print(f"OCR Error: {str(e)}")
            return ""
    
    def extract_text_with_confidence(self, image: np.ndarray) -> Dict:
        """
        Extract text with confidence scores using multi-config approach
        
        Args:
            image: Input image
            
        Returns:
            Dict with text and confidence data
        """
        try:
            # Use multi-config extraction for better results
            text, stats = self.extract_text_multi_config(image, debug=False)
            
            # Get confidence data
            data = pytesseract.image_to_data(
                image, 
                lang=self.language, 
                config=self.config,
                output_type=pytesseract.Output.DICT
            )
            
            # Filter out low confidence results
            filtered_text = []
            confidences = []
            
            for i in range(len(data['text'])):
                if int(data['conf'][i]) > 30:  # Confidence threshold
                    text_word = data['text'][i].strip()
                    if text_word:
                        filtered_text.append(text_word)
                        confidences.append(int(data['conf'][i]))
            
            avg_confidence = np.mean(confidences) if confidences else 0
            
            return {
                'text': text,  # Use multi-config text (better quality)
                'confidence': float(avg_confidence),
                'word_count': stats.get('words', len(filtered_text)),
                'all_confidences': confidences
            }
        except Exception as e:
            print(f"OCR Error: {str(e)}")
            return {'text': '', 'confidence': 0, 'word_count': 0}
    
    def extract_structured_data(self, image: np.ndarray) -> List[Dict]:
        """
        Extract structured data (boxes with text and positions)
        
        Args:
            image: Input image
            
        Returns:
            List of dicts with text and bounding box info
        """
        try:
            data = pytesseract.image_to_data(
                image,
                lang=self.language,
                config=self.config,
                output_type=pytesseract.Output.DICT
            )
            
            structured = []
            
            for i in range(len(data['text'])):
                if int(data['conf'][i]) > 30:
                    text = data['text'][i].strip()
                    if text:
                        structured.append({
                            'text': text,
                            'confidence': int(data['conf'][i]),
                            'x': data['left'][i],
                            'y': data['top'][i],
                            'width': data['width'][i],
                            'height': data['height'][i],
                            'block_num': data['block_num'][i],
                            'line_num': data['line_num'][i]
                        })
            
            return structured
        except Exception as e:
            print(f"OCR Error: {str(e)}")
            return []


class DocumentClassifier:
    """
    Classify document types using KNN classifier
    """
    
    # Document types
    DOCUMENT_TYPES = ['ID', 'BILL', 'RECEIPT', 'FORM', 'NOTE', 'OTHER']
    
    def __init__(self, model_path: Optional[str] = None):
        """
        Initialize document classifier
        
        Args:
            model_path: Path to saved model (optional)
        """
        self.model = KNeighborsClassifier(n_neighbors=3)
        self.scaler = StandardScaler()
        self.is_trained = False
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
    
    def extract_features(self, image: np.ndarray) -> np.ndarray:
        """
        Extract features from image for classification
        
        Args:
            image: Input image
            
        Returns:
            Feature vector
        """
        # Convert to grayscale
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        # Feature 1: Aspect ratio
        h, w = gray.shape
        aspect_ratio = w / h if h > 0 else 1.0
        
        # Feature 2: Text density (approximate)
        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)
        text_density = np.sum(binary > 0) / binary.size
        
        # Feature 3: Edge density
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        # Feature 4: Horizontal lines (common in bills/receipts)
        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, 
                               minLineLength=100, maxLineGap=10)
        horizontal_lines = 0
        if lines is not None:
            for line in lines:
                x1, y1, x2, y2 = line[0]
                angle = abs(np.degrees(np.arctan2(y2 - y1, x2 - x1)))
                if angle < 10 or angle > 170:  # Nearly horizontal
                    horizontal_lines += 1
        
        # Feature 5: Vertical lines (common in forms)
        vertical_lines = 0
        if lines is not None:
            for line in lines:
                x1, y1, x2, y2 = line[0]
                angle = abs(np.degrees(np.arctan2(y2 - y1, x2 - x1)))
                if 80 < angle < 100:  # Nearly vertical
                    vertical_lines += 1
        
        # Feature 6: Mean intensity
        mean_intensity = np.mean(gray)
        
        # Feature 7: Std deviation of intensity
        std_intensity = np.std(gray)
        
        # Feature 8: Contour count (complexity)
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        contour_count = len(contours)
        
        # Combine features
        features = np.array([
            aspect_ratio,
            text_density,
            edge_density,
            horizontal_lines,
            vertical_lines,
            mean_intensity,
            std_intensity,
            contour_count
        ])
        
        return features
    
    def train(self, images: List[np.ndarray], labels: List[str]):
        """
        Train the classifier
        
        Args:
            images: List of training images
            labels: List of corresponding labels
        """
        # Extract features
        features = np.array([self.extract_features(img) for img in images])
        
        # Scale features
        features_scaled = self.scaler.fit_transform(features)
        
        # Train model
        self.model.fit(features_scaled, labels)
        self.is_trained = True
    
    def predict(self, image: np.ndarray) -> Tuple[str, float]:
        """
        Predict document type
        
        Args:
            image: Input image
            
        Returns:
            Tuple of (predicted_type, confidence)
        """
        if not self.is_trained:
            return 'OTHER', 0.0
        
        # Extract and scale features
        features = self.extract_features(image).reshape(1, -1)
        features_scaled = self.scaler.transform(features)
        
        # Predict
        prediction = self.model.predict(features_scaled)[0]
        
        # Get probability/confidence
        proba = self.model.predict_proba(features_scaled)[0]
        confidence = float(np.max(proba))
        
        return prediction, confidence
    
    def save_model(self, path: str):
        """Save trained model"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'is_trained': self.is_trained
        }
        with open(path, 'wb') as f:
            pickle.dump(model_data, f)
    
    def load_model(self, path: str):
        """Load trained model"""
        with open(path, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.is_trained = model_data['is_trained']


class AIEnhancer:
    """
    AI-based image enhancement (placeholder for future ML models)
    """
    
    def __init__(self):
        """Initialize AI enhancer"""
        # Placeholder for future MobileNet or custom CNN integration
        self.model = None
    
    def enhance_quality(self, image: np.ndarray) -> np.ndarray:
        """
        Enhance image quality using AI
        
        Args:
            image: Input image
            
        Returns:
            Enhanced image
        """
        # For now, use traditional enhancement
        # TODO: Integrate MobileNet or custom trained model
        
        # Convert to LAB color space
        if len(image.shape) == 3:
            lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
            l, a, b = cv2.split(lab)
            
            # Apply CLAHE to L channel
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
            l = clahe.apply(l)
            
            # Merge and convert back
            enhanced = cv2.merge([l, a, b])
            enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)
        else:
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(image)
        
        # Apply sharpening
        kernel = np.array([[-1,-1,-1], 
                          [-1, 9,-1], 
                          [-1,-1,-1]])
        enhanced = cv2.filter2D(enhanced, -1, kernel)
        
        return enhanced
    
    def super_resolution(self, image: np.ndarray) -> np.ndarray:
        """
        Apply super-resolution (placeholder)
        
        Args:
            image: Input image
            
        Returns:
            High-resolution image
        """
        # Placeholder for future DNN super-resolution
        # For now, use bicubic interpolation
        scale_factor = 2
        height, width = image.shape[:2]
        enhanced = cv2.resize(
            image, 
            (width * scale_factor, height * scale_factor),
            interpolation=cv2.INTER_CUBIC
        )
        return enhanced


print("âœ… OCR module loaded (improved multi-config)")
