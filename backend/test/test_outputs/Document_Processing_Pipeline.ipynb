{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Document Processing Pipeline\n",
    "\n",
    "**12-Step CamScanner-style Document Processing**\n",
    "\n",
    "This notebook demonstrates our complete document processing pipeline, broken down into 12 detailed steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Configure display\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Paths\n",
    "TEST_DIR = os.path.dirname(os.path.abspath('__file__')) if '__file__' in dir() else os.getcwd()\n",
    "if 'test_outputs' not in TEST_DIR:\n",
    "    TEST_DIR = os.path.join(TEST_DIR, 'test_outputs') if os.path.exists(os.path.join(TEST_DIR, 'test_outputs')) else TEST_DIR\n",
    "\n",
    "print(f\"Working directory: {TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions-header",
   "metadata": {},
   "source": [
    "## Processing Functions\n",
    "\n",
    "Core functions for document processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "    \"\"\"Order points: top-left, top-right, bottom-right, bottom-left\"\"\"\n",
    "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
    "    s = pts.sum(axis=1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    diff = np.diff(pts, axis=1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    return rect\n",
    "\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "    \"\"\"Apply perspective transform\"\"\"\n",
    "    rect = order_points(pts)\n",
    "    (tl, tr, br, bl) = rect\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    dst = np.array([[0, 0], [maxWidth - 1, 0], [maxWidth - 1, maxHeight - 1], [0, maxHeight - 1]], dtype=\"float32\")\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    return cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "\n",
    "\n",
    "def find_document(image):\n",
    "    \"\"\"Detect document boundaries\"\"\"\n",
    "    scale = 0.25\n",
    "    small = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n",
    "    gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    for low, high in [(30, 100), (50, 150), (75, 200)]:\n",
    "        edges = cv2.Canny(blurred, low, high)\n",
    "        edges = cv2.dilate(edges, np.ones((2, 2)), iterations=2)\n",
    "        contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]\n",
    "        \n",
    "        for c in contours:\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            if len(approx) == 4:\n",
    "                area = cv2.contourArea(approx)\n",
    "                if area > small.shape[0] * small.shape[1] * 0.1:\n",
    "                    return (approx.reshape(4, 2) / scale).astype(np.float32)\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"✓ Processing functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1: Load Original Image\n",
    "\n",
    "Load the raw image from camera/scanner input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load original image\n",
    "image_path = os.path.join(TEST_DIR, 'original.jpg')\n",
    "if not os.path.exists(image_path):\n",
    "    # Try parent directory\n",
    "    image_path = os.path.join(os.path.dirname(TEST_DIR), 'original.jpg')\n",
    "\n",
    "original = cv2.imread(image_path)\n",
    "if original is None:\n",
    "    print(f\"Error: Could not load image from {image_path}\")\n",
    "else:\n",
    "    print(f\"✓ Loaded image: {original.shape[1]}x{original.shape[0]} pixels\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Step 1: Original Image', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2: Downscale for Detection\n",
    "\n",
    "Downscale the image for faster edge detection processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Downscale for detection\n",
    "scale = 0.25\n",
    "small = cv2.resize(original, (0, 0), fx=scale, fy=scale)\n",
    "print(f\"✓ Downscaled to: {small.shape[1]}x{small.shape[0]} pixels (scale={scale})\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cv2.cvtColor(small, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Step 2: Downscaled Image for Detection', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3: Convert to Grayscale (Detection)\n",
    "\n",
    "Convert the downscaled image to grayscale for edge detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert to grayscale for detection\n",
    "gray_small = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
    "print(f\"✓ Converted to grayscale\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(gray_small, cmap='gray')\n",
    "plt.title('Step 3: Grayscale (for edge detection)', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4: Gaussian Blur\n",
    "\n",
    "Apply Gaussian blur to reduce noise before edge detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Gaussian blur\n",
    "blurred = cv2.GaussianBlur(gray_small, (5, 5), 0)\n",
    "print(f\"✓ Applied Gaussian blur (5x5)\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(blurred, cmap='gray')\n",
    "plt.title('Step 4: Gaussian Blur', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5: Canny Edge Detection\n",
    "\n",
    "Apply Canny edge detection to find document boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Canny edge detection\n",
    "edges = cv2.Canny(blurred, 50, 150)\n",
    "edges_dilated = cv2.dilate(edges, np.ones((2, 2)), iterations=2)\n",
    "print(f\"✓ Canny edge detection applied (50, 150)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(edges, cmap='gray')\n",
    "axes[0].set_title('Canny Edges (Raw)', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(edges_dilated, cmap='gray')\n",
    "axes[1].set_title('Edges (Dilated)', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "plt.suptitle('Step 5: Edge Detection', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 6: Find Document Contour\n",
    "\n",
    "Find the largest quadrilateral contour representing the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Find document contour\n",
    "pts = find_document(original)\n",
    "\n",
    "vis = original.copy()\n",
    "if pts is not None:\n",
    "    pts_int = pts.astype(int)\n",
    "    cv2.polylines(vis, [pts_int], True, (0, 255, 0), 3)\n",
    "    for i, pt in enumerate(pts_int):\n",
    "        cv2.circle(vis, tuple(pt), 10, (0, 0, 255), -1)\n",
    "        cv2.putText(vis, str(i+1), tuple(pt + 15), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    print(f\"✓ Document detected with 4 corners\")\n",
    "else:\n",
    "    print(\"! No document detected\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Step 6: Document Contour Detection', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-header",
   "metadata": {},
   "source": [
    "## Step 7: Perspective Transform (Crop)\n",
    "\n",
    "Apply perspective transformation to get a bird's-eye view of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Perspective transform\n",
    "if pts is not None:\n",
    "    warped = four_point_transform(original, pts)\n",
    "    print(f\"✓ Perspective corrected: {warped.shape[1]}x{warped.shape[0]} pixels\")\n",
    "else:\n",
    "    warped = original\n",
    "    print(\"! Using original image (no document detected)\")\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Step 7: Perspective Corrected (Cropped)', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-header",
   "metadata": {},
   "source": [
    "## Step 8: Convert to Grayscale (Processing)\n",
    "\n",
    "Convert the cropped document to grayscale for enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Convert to grayscale\n",
    "gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "print(f\"✓ Converted to grayscale: {gray.shape[1]}x{gray.shape[0]}\")\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.imshow(gray, cmap='gray')\n",
    "plt.title('Step 8: Grayscale Document', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-header",
   "metadata": {},
   "source": [
    "## Step 9: Downscale for Background Estimation\n",
    "\n",
    "Downscale for faster morphological operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Downscale for background estimation\n",
    "h, w = gray.shape\n",
    "max_dim = 800\n",
    "bg_scale = min(max_dim / max(h, w), 1.0)\n",
    "\n",
    "if bg_scale < 1.0:\n",
    "    small_gray = cv2.resize(gray, (int(w * bg_scale), int(h * bg_scale)))\n",
    "else:\n",
    "    small_gray = gray\n",
    "\n",
    "print(f\"✓ Background processing scale: {bg_scale:.2f}\")\n",
    "print(f\"  Downscaled to: {small_gray.shape[1]}x{small_gray.shape[0]}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(small_gray, cmap='gray')\n",
    "plt.title('Step 9: Downscaled for Background Estimation', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step10-header",
   "metadata": {},
   "source": [
    "## Step 10: Morphological Background Estimation\n",
    "\n",
    "Use morphological closing to estimate the background illumination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Morphological background estimation\n",
    "sh, sw = small_gray.shape\n",
    "k = max(sh, sw) // 8\n",
    "k = k if k % 2 == 1 else k + 1\n",
    "k = max(31, min(k, 127))\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "bg_raw = cv2.morphologyEx(small_gray, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "print(f\"✓ Background estimated with kernel size: {k}x{k}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "axes[0].imshow(small_gray, cmap='gray')\n",
    "axes[0].set_title('Original Grayscale', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(bg_raw, cmap='gray')\n",
    "axes[1].set_title('Background Estimation (Morphological Close)', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "plt.suptitle('Step 10: Background Estimation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step11-header",
   "metadata": {},
   "source": [
    "## Step 11: Smooth and Upscale Background\n",
    "\n",
    "Smooth the background and upscale back to original resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Smooth and upscale background\n",
    "bg_smoothed = cv2.GaussianBlur(bg_raw, (k, k), 0)\n",
    "\n",
    "if bg_scale < 1.0:\n",
    "    bg = cv2.resize(bg_smoothed, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    bg = cv2.GaussianBlur(bg, (31, 31), 0)\n",
    "else:\n",
    "    bg = bg_smoothed\n",
    "\n",
    "print(f\"✓ Background smoothed and upscaled to: {bg.shape[1]}x{bg.shape[0]}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "axes[0].imshow(bg_raw, cmap='gray')\n",
    "axes[0].set_title('Raw Background', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(bg, cmap='gray')\n",
    "axes[1].set_title('Smoothed Background (Full Resolution)', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "plt.suptitle('Step 11: Background Smoothing', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step12-header",
   "metadata": {},
   "source": [
    "## Step 12: Shadow Removal (Final Output)\n",
    "\n",
    "Normalize illumination by dividing by background to remove shadows.\n",
    "\n",
    "**Formula:** `result = (pixel / background) × 255`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Shadow removal (illumination normalization)\n",
    "result = np.zeros_like(gray, dtype=np.float32)\n",
    "mask = bg > 10\n",
    "result[mask] = (gray[mask].astype(np.float32) / bg[mask].astype(np.float32)) * 255\n",
    "result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "print(f\"✓ Shadow removal complete\")\n",
    "print(f\"  Formula: result = (pixel / background) × 255\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 8))\n",
    "axes[0].imshow(gray, cmap='gray')\n",
    "axes[0].set_title('Input (with shadows)', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(bg, cmap='gray')\n",
    "axes[1].set_title('Background Estimation', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(result, cmap='gray')\n",
    "axes[2].set_title('Final Output (shadows removed)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "plt.suptitle('Step 12: Shadow Removal (Final Output)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Pipeline Summary\n",
    "\n",
    "Complete overview of all 12 processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline summary visualization\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "# Row 1: Steps 1-4\n",
    "axes[0, 0].imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('1. Load Original', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(cv2.cvtColor(small, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 1].set_title('2. Downscale', fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(gray_small, cmap='gray')\n",
    "axes[0, 2].set_title('3. Grayscale (detect)', fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[0, 3].imshow(blurred, cmap='gray')\n",
    "axes[0, 3].set_title('4. Gaussian Blur', fontweight='bold')\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "# Row 2: Steps 5-8\n",
    "axes[1, 0].imshow(edges_dilated, cmap='gray')\n",
    "axes[1, 0].set_title('5. Canny Edges', fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 1].set_title('6. Find Contour', fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 2].set_title('7. Perspective Correct', fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "axes[1, 3].imshow(gray, cmap='gray')\n",
    "axes[1, 3].set_title('8. Grayscale (doc)', fontweight='bold')\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "# Row 3: Steps 9-12\n",
    "axes[2, 0].imshow(small_gray, cmap='gray')\n",
    "axes[2, 0].set_title('9. Downscale (bg)', fontweight='bold')\n",
    "axes[2, 0].axis('off')\n",
    "\n",
    "axes[2, 1].imshow(bg_raw, cmap='gray')\n",
    "axes[2, 1].set_title('10. Morph Background', fontweight='bold')\n",
    "axes[2, 1].axis('off')\n",
    "\n",
    "axes[2, 2].imshow(bg, cmap='gray')\n",
    "axes[2, 2].set_title('11. Smooth/Upscale', fontweight='bold')\n",
    "axes[2, 2].axis('off')\n",
    "\n",
    "axes[2, 3].imshow(result, cmap='gray')\n",
    "axes[2, 3].set_title('12. Shadow Removed', fontweight='bold')\n",
    "axes[2, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Document Processing Pipeline - 12 Steps', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE SUMMARY - 12 STEPS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "| Step | Stage                      | Description                    |\n",
    "|------|----------------------------|--------------------------------|\n",
    "| 1    | Load Original              | Load raw image from input      |\n",
    "| 2    | Downscale (Detection)      | Reduce size for faster edges   |\n",
    "| 3    | Grayscale (Detection)      | Convert to grayscale           |\n",
    "| 4    | Gaussian Blur              | Reduce noise for edge detect   |\n",
    "| 5    | Canny Edge Detection       | Find edges in image            |\n",
    "| 6    | Find Document Contour      | Locate 4-corner document       |\n",
    "| 7    | Perspective Transform      | Correct skew, crop document    |\n",
    "| 8    | Grayscale (Processing)     | Convert cropped doc to gray    |\n",
    "| 9    | Downscale (Background)     | Reduce for morphology ops      |\n",
    "| 10   | Morphological Background   | Estimate background lighting   |\n",
    "| 11   | Smooth/Upscale Background  | Smooth and restore resolution  |\n",
    "| 12   | Shadow Removal (Output)    | Normalize illumination         |\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
