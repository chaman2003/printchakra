{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7123378",
   "metadata": {},
   "source": [
    "# üì∏ Image Processing Pipeline - Step by Step\n",
    "\n",
    "This notebook demonstrates the complete image processing pipeline used in **PrintChakra**, from raw image input to final processed output.\n",
    "\n",
    "## Overview\n",
    "- **Input**: `image.jpg` (raw document photo)\n",
    "- **Output**: Processed, enhanced document image\n",
    "- **Pipeline Steps**: 12 stages of image processing\n",
    "- **Each step shows**: Input ‚Üí Processing ‚Üí Output ‚Üí Debug Info\n",
    "\n",
    "## Requirements\n",
    "- OpenCV (cv2)\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- PIL/Pillow\n",
    "- pytesseract (for OCR)\n",
    "\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c893700",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for image processing, visualization, and OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure matplotlib for inline display\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Print library versions\n",
    "print(\"=\" * 60)\n",
    "print(\"üì¶ LIBRARY VERSIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {os.sys.version.split()[0]}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a8743",
   "metadata": {},
   "source": [
    "## Step 2: Load and Display Input Image\n",
    "\n",
    "Load the input image `image.jpg` from the current directory and verify it was loaded successfully.\n",
    "\n",
    "**Expected Input**: File path to `image.jpg`  \n",
    "**Expected Output**: Loaded image as NumPy array with shape (height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: File path\n",
    "input_image_path = 'image.jpg'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìÇ LOADING INPUT IMAGE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input file: {input_image_path}\")\n",
    "\n",
    "# Load image\n",
    "try:\n",
    "    original_image = cv2.imread(input_image_path)\n",
    "    \n",
    "    if original_image is None:\n",
    "        raise FileNotFoundError(f\"‚ùå Error: Could not load image from '{input_image_path}'\")\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"‚úÖ Image loaded successfully!\")\n",
    "    print(f\"Image shape: {original_image.shape} (height, width, channels)\")\n",
    "    print(f\"Data type: {original_image.dtype}\")\n",
    "    print(f\"File size: {os.path.getsize(input_image_path) / 1024:.2f} KB\")\n",
    "    print(f\"Color space: BGR (OpenCV default)\")\n",
    "    \n",
    "    # Convert BGR to RGB for display\n",
    "    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(original_image_rgb)\n",
    "    plt.title('Original Input Image', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: {str(e)}\")\n",
    "    print(\"Please ensure 'image.jpg' exists in the current directory\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006e24b",
   "metadata": {},
   "source": [
    "## Step 3: Convert to Grayscale\n",
    "\n",
    "Convert the BGR color image to grayscale. This reduces the image from 3 channels (BGR) to 1 channel (grayscale), simplifying further processing.\n",
    "\n",
    "**Expected Input**: BGR image (height, width, 3)  \n",
    "**Expected Output**: Grayscale image (height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879821a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: BGR color image\n",
    "print(\"=\" * 60)\n",
    "print(\"üé® GRAYSCALE CONVERSION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {original_image.shape}\")\n",
    "print(f\"Input channels: {original_image.shape[2]}\")\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Debug info\n",
    "print(f\"Output shape: {gray.shape}\")\n",
    "print(f\"Output channels: 1 (grayscale)\")\n",
    "print(f\"Data type: {gray.dtype}\")\n",
    "print(f\"Value range: [{gray.min()}, {gray.max()}]\")\n",
    "print(f\"Mean intensity: {gray.mean():.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(original_image_rgb)\n",
    "axes[0].set_title('Input: Color Image', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(gray, cmap='gray')\n",
    "axes[1].set_title('Output: Grayscale Image', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Grayscale conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979a67f",
   "metadata": {},
   "source": [
    "## Step 4: Apply Gaussian Blur\n",
    "\n",
    "Apply Gaussian blur to reduce noise and smooth the image. This helps improve edge detection in later steps.\n",
    "\n",
    "**Expected Input**: Grayscale image  \n",
    "**Expected Output**: Blurred grayscale image with reduced noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eade9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Grayscale image\n",
    "kernel_size = (5, 5)\n",
    "sigma = 0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üå´Ô∏è  GAUSSIAN BLUR\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {gray.shape}\")\n",
    "print(f\"Kernel size: {kernel_size}\")\n",
    "print(f\"Sigma: {sigma} (auto-calculated)\")\n",
    "\n",
    "# Apply Gaussian blur\n",
    "blurred = cv2.GaussianBlur(gray, kernel_size, sigma)\n",
    "\n",
    "# Debug info\n",
    "print(f\"Output shape: {blurred.shape}\")\n",
    "print(f\"Blur effect - before mean: {gray.mean():.2f}, after mean: {blurred.mean():.2f}\")\n",
    "print(f\"Standard deviation - before: {gray.std():.2f}, after: {blurred.std():.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(gray, cmap='gray')\n",
    "axes[0].set_title('Input: Sharp Grayscale', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(blurred, cmap='gray')\n",
    "axes[1].set_title(f'Output: Blurred (kernel={kernel_size})', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Gaussian blur applied successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85fee41",
   "metadata": {},
   "source": [
    "## Step 5: Edge Detection (Canny)\n",
    "\n",
    "Apply Canny edge detection to find edges in the image. This is crucial for detecting document boundaries.\n",
    "\n",
    "**Expected Input**: Blurred grayscale image  \n",
    "**Expected Output**: Binary edge map (white edges on black background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ffd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Blurred grayscale image\n",
    "threshold1 = 50\n",
    "threshold2 = 150\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç CANNY EDGE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {blurred.shape}\")\n",
    "print(f\"Lower threshold: {threshold1}\")\n",
    "print(f\"Upper threshold: {threshold2}\")\n",
    "\n",
    "# Apply Canny edge detection\n",
    "edges = cv2.Canny(blurred, threshold1, threshold2)\n",
    "\n",
    "# Debug info\n",
    "print(f\"Output shape: {edges.shape}\")\n",
    "print(f\"Output range: [{edges.min()}, {edges.max()}]\")\n",
    "print(f\"Edge pixels: {np.count_nonzero(edges)} ({100 * np.count_nonzero(edges) / edges.size:.2f}%)\")\n",
    "print(f\"Non-edge pixels: {edges.size - np.count_nonzero(edges)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(blurred, cmap='gray')\n",
    "axes[0].set_title('Input: Blurred Image', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(edges, cmap='gray')\n",
    "axes[1].set_title(f'Output: Detected Edges (t1={threshold1}, t2={threshold2})', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Edge detection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51954bb",
   "metadata": {},
   "source": [
    "## Step 6: Binary Thresholding\n",
    "\n",
    "Convert the grayscale image to a pure binary image (black and white only) using Otsu's thresholding method.\n",
    "\n",
    "**Expected Input**: Grayscale image  \n",
    "**Expected Output**: Binary image (0 or 255 values only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Grayscale image\n",
    "print(\"=\" * 60)\n",
    "print(\"‚ö´‚ö™ BINARY THRESHOLDING (Otsu's Method)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {gray.shape}\")\n",
    "print(f\"Input value range: [{gray.min()}, {gray.max()}]\")\n",
    "\n",
    "# Apply Otsu's thresholding\n",
    "threshold_value, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Debug info\n",
    "print(f\"Auto-calculated threshold: {threshold_value:.2f}\")\n",
    "print(f\"Output shape: {binary.shape}\")\n",
    "print(f\"Output values: {np.unique(binary)} (only 0 and 255)\")\n",
    "print(f\"White pixels: {np.count_nonzero(binary)} ({100 * np.count_nonzero(binary) / binary.size:.2f}%)\")\n",
    "print(f\"Black pixels: {binary.size - np.count_nonzero(binary)} ({100 * (1 - np.count_nonzero(binary) / binary.size):.2f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(gray, cmap='gray')\n",
    "axes[0].set_title('Input: Grayscale (256 levels)', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(binary, cmap='gray')\n",
    "axes[1].set_title(f'Output: Binary (threshold={threshold_value:.0f})', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Binary thresholding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f4621",
   "metadata": {},
   "source": [
    "## Step 7: Morphological Operations (Erosion & Dilation)\n",
    "\n",
    "Apply morphological operations to clean up the binary image by removing small noise and filling gaps.\n",
    "\n",
    "**Expected Input**: Binary image  \n",
    "**Expected Output**: Cleaned binary image with noise removed and gaps filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae88504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Binary image\n",
    "kernel_size = (3, 3)\n",
    "iterations = 1\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üî® MORPHOLOGICAL OPERATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {binary.shape}\")\n",
    "print(f\"Kernel size: {kernel_size}\")\n",
    "print(f\"Iterations: {iterations}\")\n",
    "\n",
    "# Create kernel\n",
    "kernel = np.ones(kernel_size, np.uint8)\n",
    "\n",
    "# Erosion (removes small white noise)\n",
    "eroded = cv2.erode(binary, kernel, iterations=iterations)\n",
    "print(f\"\\n1Ô∏è‚É£ Erosion complete\")\n",
    "print(f\"   White pixels before: {np.count_nonzero(binary)}\")\n",
    "print(f\"   White pixels after: {np.count_nonzero(eroded)}\")\n",
    "print(f\"   Pixels removed: {np.count_nonzero(binary) - np.count_nonzero(eroded)}\")\n",
    "\n",
    "# Dilation (fills small black gaps)\n",
    "dilated = cv2.dilate(eroded, kernel, iterations=iterations)\n",
    "print(f\"\\n2Ô∏è‚É£ Dilation complete\")\n",
    "print(f\"   White pixels before: {np.count_nonzero(eroded)}\")\n",
    "print(f\"   White pixels after: {np.count_nonzero(dilated)}\")\n",
    "print(f\"   Pixels added: {np.count_nonzero(dilated) - np.count_nonzero(eroded)}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "axes[0].imshow(binary, cmap='gray')\n",
    "axes[0].set_title('Input: Original Binary', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(eroded, cmap='gray')\n",
    "axes[1].set_title('After Erosion (noise removed)', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(dilated, cmap='gray')\n",
    "axes[2].set_title('After Dilation (gaps filled)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Morphological operations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b408dd",
   "metadata": {},
   "source": [
    "## Step 8: Contour Detection\n",
    "\n",
    "Find and draw contours (outlines) of objects in the image. This helps identify document boundaries.\n",
    "\n",
    "**Expected Input**: Binary image  \n",
    "**Expected Output**: List of contours and image with contours drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Binary/edge image\n",
    "print(\"=\" * 60)\n",
    "print(\"üìê CONTOUR DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {edges.shape}\")\n",
    "\n",
    "# Find contours\n",
    "contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Debug info\n",
    "print(f\"Total contours found: {len(contours)}\")\n",
    "\n",
    "# Sort by area (largest first)\n",
    "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# Display info about top 5 largest contours\n",
    "print(f\"\\nTop 5 largest contours:\")\n",
    "for i, cnt in enumerate(sorted_contours[:5]):\n",
    "    area = cv2.contourArea(cnt)\n",
    "    perimeter = cv2.arcLength(cnt, True)\n",
    "    print(f\"  {i+1}. Area: {area:.0f} px¬≤, Perimeter: {perimeter:.0f} px\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Draw contours on color image\n",
    "contour_image = original_image.copy()\n",
    "cv2.drawContours(contour_image, sorted_contours[:10], -1, (0, 255, 0), 3)\n",
    "contour_image_rgb = cv2.cvtColor(contour_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(edges, cmap='gray')\n",
    "axes[0].set_title('Input: Edge Map', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(contour_image_rgb)\n",
    "axes[1].set_title(f'Output: Top 10 Contours (total: {len(contours)})', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Contour detection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab2627",
   "metadata": {},
   "source": [
    "## Step 9: Image Resizing and Scaling\n",
    "\n",
    "Resize the image to specific dimensions. This is useful for standardizing output or reducing file size.\n",
    "\n",
    "**Expected Input**: Processed image  \n",
    "**Expected Output**: Resized image with new dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Grayscale image\n",
    "target_width = 800\n",
    "scale_percent = None  # Or specify percentage\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìè IMAGE RESIZING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original dimensions: {gray.shape[1]}x{gray.shape[0]} (width x height)\")\n",
    "\n",
    "# Calculate new dimensions maintaining aspect ratio\n",
    "aspect_ratio = gray.shape[0] / gray.shape[1]\n",
    "target_height = int(target_width * aspect_ratio)\n",
    "\n",
    "print(f\"Target width: {target_width}px\")\n",
    "print(f\"Calculated height: {target_height}px (maintaining aspect ratio)\")\n",
    "print(f\"Interpolation method: INTER_LINEAR\")\n",
    "\n",
    "# Resize image\n",
    "resized = cv2.resize(gray, (target_width, target_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# Debug info\n",
    "original_pixels = gray.shape[0] * gray.shape[1]\n",
    "resized_pixels = resized.shape[0] * resized.shape[1]\n",
    "scale_factor = resized_pixels / original_pixels\n",
    "\n",
    "print(f\"\\nOutput dimensions: {resized.shape[1]}x{resized.shape[0]}\")\n",
    "print(f\"Original pixels: {original_pixels:,}\")\n",
    "print(f\"Resized pixels: {resized_pixels:,}\")\n",
    "print(f\"Scale factor: {scale_factor:.2%}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(gray, cmap='gray')\n",
    "axes[0].set_title(f'Input: {gray.shape[1]}x{gray.shape[0]}', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(resized, cmap='gray')\n",
    "axes[1].set_title(f'Output: {resized.shape[1]}x{resized.shape[0]}', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Image resizing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1ea1c",
   "metadata": {},
   "source": [
    "## Step 10: Histogram Equalization\n",
    "\n",
    "Apply histogram equalization to enhance contrast and improve image quality for better OCR results.\n",
    "\n",
    "**Expected Input**: Grayscale image  \n",
    "**Expected Output**: Contrast-enhanced image with more uniform intensity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ada073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Grayscale image\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä HISTOGRAM EQUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {gray.shape}\")\n",
    "print(f\"Input value range: [{gray.min()}, {gray.max()}]\")\n",
    "print(f\"Input mean: {gray.mean():.2f}, std: {gray.std():.2f}\")\n",
    "\n",
    "# Apply histogram equalization\n",
    "equalized = cv2.equalizeHist(gray)\n",
    "\n",
    "# Debug info\n",
    "print(f\"\\nOutput value range: [{equalized.min()}, {equalized.max()}]\")\n",
    "print(f\"Output mean: {equalized.mean():.2f}, std: {equalized.std():.2f}\")\n",
    "print(f\"Contrast improvement: {equalized.std() / gray.std():.2f}x\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create figure with images and histograms\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Original image\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "ax1.imshow(gray, cmap='gray')\n",
    "ax1.set_title('Input: Original Grayscale', fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Original histogram\n",
    "ax2 = plt.subplot(2, 2, 2)\n",
    "ax2.hist(gray.ravel(), bins=256, range=[0, 256], color='blue', alpha=0.7)\n",
    "ax2.set_title('Input Histogram', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Pixel Intensity')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Equalized image\n",
    "ax3 = plt.subplot(2, 2, 3)\n",
    "ax3.imshow(equalized, cmap='gray')\n",
    "ax3.set_title('Output: Equalized (Enhanced Contrast)', fontsize=12, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "# Equalized histogram\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "ax4.hist(equalized.ravel(), bins=256, range=[0, 256], color='green', alpha=0.7)\n",
    "ax4.set_title('Output Histogram (More Uniform)', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Pixel Intensity')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Histogram equalization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ca6e3",
   "metadata": {},
   "source": [
    "## Step 11: OCR Text Extraction (Optional)\n",
    "\n",
    "Extract text from the processed image using Tesseract OCR.\n",
    "\n",
    "**Expected Input**: Processed grayscale/binary image  \n",
    "**Expected Output**: Extracted text string\n",
    "\n",
    "**Note**: Requires Tesseract OCR to be installed on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc3470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Processed image (use equalized for better results)\n",
    "print(\"=\" * 60)\n",
    "print(\"üìù OCR TEXT EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input shape: {equalized.shape}\")\n",
    "\n",
    "try:\n",
    "    # Perform OCR\n",
    "    text = pytesseract.image_to_string(equalized, lang='eng')\n",
    "    \n",
    "    # Debug info\n",
    "    text_length = len(text.strip())\n",
    "    word_count = len(text.split())\n",
    "    line_count = len([line for line in text.split('\\n') if line.strip()])\n",
    "    \n",
    "    print(f\"‚úÖ OCR completed successfully!\")\n",
    "    print(f\"Characters extracted: {text_length}\")\n",
    "    print(f\"Word count: {word_count}\")\n",
    "    print(f\"Line count: {line_count}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Display extracted text\n",
    "    print(\"\\nüìÑ EXTRACTED TEXT:\")\n",
    "    print(\"-\" * 60)\n",
    "    if text.strip():\n",
    "        print(text)\n",
    "    else:\n",
    "        print(\"(No text detected)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get additional OCR data\n",
    "    data = pytesseract.image_to_data(equalized, output_type=pytesseract.Output.DICT)\n",
    "    n_boxes = len(data['text'])\n",
    "    confident_words = [i for i in range(n_boxes) if int(data['conf'][i]) > 60]\n",
    "    \n",
    "    print(f\"\\nConfidence analysis:\")\n",
    "    print(f\"Total words detected: {n_boxes}\")\n",
    "    print(f\"High confidence (>60%): {len(confident_words)} words\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå OCR Error: {str(e)}\")\n",
    "    print(\"Make sure Tesseract OCR is installed and in your PATH\")\n",
    "    print(\"Windows: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0b0a9",
   "metadata": {},
   "source": [
    "## Step 12: Save Final Processed Image\n",
    "\n",
    "Save the processed image to disk for later use.\n",
    "\n",
    "**Expected Input**: Final processed image  \n",
    "**Expected Output**: Saved image file `output.jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a057f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Final processed image (using equalized as final output)\n",
    "output_filename = 'output.jpg'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üíæ SAVING PROCESSED IMAGE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Output filename: {output_filename}\")\n",
    "print(f\"Image shape: {equalized.shape}\")\n",
    "print(f\"Data type: {equalized.dtype}\")\n",
    "\n",
    "# Save the image\n",
    "try:\n",
    "    cv2.imwrite(output_filename, equalized)\n",
    "    \n",
    "    # Verify file was saved\n",
    "    if os.path.exists(output_filename):\n",
    "        file_size = os.path.getsize(output_filename)\n",
    "        print(f\"‚úÖ Image saved successfully!\")\n",
    "        print(f\"File size: {file_size / 1024:.2f} KB\")\n",
    "        print(f\"Full path: {os.path.abspath(output_filename)}\")\n",
    "        \n",
    "        # Display before/after comparison\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        axes[0].imshow(original_image_rgb)\n",
    "        axes[0].set_title('BEFORE: Original Input', fontsize=12, fontweight='bold')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(equalized, cmap='gray')\n",
    "        axes[1].set_title('AFTER: Final Processed Output', fontsize=12, fontweight='bold')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Error: File was not saved\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Save Error: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc05199",
   "metadata": {},
   "source": [
    "## üéâ Processing Complete!\n",
    "\n",
    "### Summary of Pipeline Steps\n",
    "\n",
    "1. ‚úÖ **Load Image** - Loaded `image.jpg` and verified integrity\n",
    "2. ‚úÖ **Grayscale Conversion** - Reduced from 3 channels to 1\n",
    "3. ‚úÖ **Gaussian Blur** - Reduced noise for better edge detection\n",
    "4. ‚úÖ **Edge Detection** - Applied Canny algorithm to find boundaries\n",
    "5. ‚úÖ **Binary Thresholding** - Converted to black & white using Otsu's method\n",
    "6. ‚úÖ **Morphological Ops** - Cleaned image with erosion & dilation\n",
    "7. ‚úÖ **Contour Detection** - Found document boundaries\n",
    "8. ‚úÖ **Resizing** - Standardized dimensions\n",
    "9. ‚úÖ **Histogram Equalization** - Enhanced contrast\n",
    "10. ‚úÖ **OCR Extraction** - Extracted text using Tesseract\n",
    "11. ‚úÖ **Save Output** - Saved final processed image\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Use `output.jpg` as your processed image\n",
    "- Adjust parameters in individual cells for better results\n",
    "- Experiment with different threshold values\n",
    "- Try different morphological kernel sizes\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **Image not loading?** Ensure `image.jpg` exists in the current directory\n",
    "- **OCR errors?** Install Tesseract OCR and add to PATH\n",
    "- **Poor results?** Adjust blur kernel size, Canny thresholds, or binary threshold\n",
    "\n",
    "---\n",
    "\n",
    "**PrintChakra Image Processing Pipeline** - Version 2.0.0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
